version: '3.8'
services:
  ollama:
    image: ollama/ollama:0.14.3
    container_name: ollama
    environment:
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=3
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    entrypoint: ["/bin/bash", "-c", "\
      ollama serve & \
      sleep 5 && \
      ollama pull llama3.1:8b && \
      wait"]
    # Configuration to use the GPU Nvidia
    #deploy:
    #  resources:
    #    reservations:
    #      devices:
    #        - driver: nvidia
    #          count: all # Use all available GPUs
    #          capabilities: [gpu]
    # restart: always # Para que se reinicie autom√°ticamente
    networks:
      - ollama-network

  #open-webui:
  #  image: ghcr.io/open-webui/open-webui:main
  #  container_name: open-webui
  #  ports:
  #    - "3000:8080"
  #  volumes:
  #    - openwebui_data:/app/backend/data
  #  environment:
  #    - OLLAMA_BASE_URL=http://ollama:11434
  #  depends_on:
  #    - ollama
  #  networks:
  #    - ollama-network
    # restart: always

volumes:
  ollama_data:
  openwebui_data:

networks:
  ollama-network:
    driver: bridge
